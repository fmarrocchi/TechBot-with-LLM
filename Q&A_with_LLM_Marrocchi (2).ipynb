{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TechBot\n",
        "\n",
        "**Author:** Florencia Marrocchi\n",
        "\n",
        "TechBot is a technical question-answering bot specialized in GCP AI tools. It leverages a knowledge base obtained from the internal system called YAQS (Yet Another Question System) to provide accurate responses. The main objective of this project is to gather specific topic-related answers in the expected format, serving both regular users and technical personnel responsible for addressing user inquiries. Throughout the development process, various prompt tuning tests are conducted to compare the obtained results. These tests aim to refine the prompts used by TechBot and improve the quality and relevance of the answers. By continuously evaluating and optimizing the system.\n"
      ],
      "metadata": {
        "id": "VrLp3-HA_YoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  0.Initial Config\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RQyAvh4OfZqb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aXbdax5vfQaH",
        "outputId": "90644223-eeac-4572-e9df-c3cdcd8399db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping google-cloud-aiplatform as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting google-cloud-aiplatform\n",
            "  Downloading google_cloud_aiplatform-1.26.1-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.11.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.22.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.20.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (23.1)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.9.0)\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform)\n",
            "  Downloading google_cloud_resource_manager-1.10.1-py2.py3-none-any.whl (321 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.3/321.3 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shapely<2.0.0 (from google-cloud-aiplatform)\n",
            "  Downloading Shapely-1.8.5.post1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.59.1)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.17.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.27.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.54.2)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.2)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
            "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform)\n",
            "  Downloading grpc_google_iam_v1-0.12.6-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.5.0)\n",
            "Installing collected packages: shapely, grpc-google-iam-v1, google-cloud-resource-manager, google-cloud-aiplatform\n",
            "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed google-cloud-aiplatform-1.26.1 google-cloud-resource-manager-1.10.1 grpc-google-iam-v1-0.12.6 shapely-1.8.5.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.35.2-py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles (from gradio)\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting aiohttp (from gradio)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.98.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client>=0.2.7 (from gradio)\n",
            "  Downloading gradio_client-0.2.7-py3-none-any.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.4/288.4 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.14.0 (from gradio)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.0.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gradio) (1.22.4)\n",
            "Collecting orjson (from gradio)\n",
            "  Downloading orjson-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio) (8.4.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from gradio) (1.10.9)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: pygments>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.14.0)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gradio) (2.27.1)\n",
            "Collecting semantic-version (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.0 (from gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (2023.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (23.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.12.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.65.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio)\n",
            "  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio) (2022.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.3)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->gradio)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->gradio)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->gradio)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->gradio)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->gradio)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (2023.5.7)\n",
            "Collecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio)\n",
            "  Downloading httpcore-0.17.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (3.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gradio) (1.26.16)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.7.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
            "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio)\n",
            "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->gradio) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio) (1.1.1)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4694 sha256=771ece1b9a28ba5aef276c81f6192b54b9764458bc4be3a5e7c08772b732c55a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/c2/0e/3b9c6845c6a4e35beb90910cc70d9ac9ab5d47402bd62af0df\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uc-micro-py, semantic-version, python-multipart, orjson, multidict, markdown-it-py, h11, frozenlist, async-timeout, aiofiles, yarl, uvicorn, starlette, mdit-py-plugins, linkify-it-py, huggingface-hub, httpcore, aiosignal, httpx, fastapi, aiohttp, gradio-client, gradio\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "Successfully installed aiofiles-23.1.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 fastapi-0.98.0 ffmpy-0.3.0 frozenlist-1.3.3 gradio-3.35.2 gradio-client-0.2.7 h11-0.14.0 httpcore-0.17.2 httpx-0.24.1 huggingface-hub-0.15.1 linkify-it-py-2.0.2 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 multidict-6.0.4 orjson-3.9.1 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.27.0 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.3 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "! pip uninstall -y google-cloud-aiplatform\n",
        "! pip install google-cloud-aiplatform --upgrade --user\n",
        "# Install gradio for UI\n",
        "! pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart kernel after installs so that the environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rUXYEi87Yf0",
        "outputId": "39f909eb-ea16-4b2f-bbf0-333f559e59eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate\n",
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()"
      ],
      "metadata": {
        "id": "1WYswCYAfZUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set parameters\n",
        "PROJECT_ID = \"cloud-llm-preview4\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "MODEL = \"text-bison@001\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "TJzd5JVn89Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Data extraction from YAQS\n",
        "Using YAQS the internal app of google, obtain examples of technical questions to add to the prompt of the model.\n",
        "\n",
        "\n",
        "Query used to get the content:\n",
        "\n",
        "*SELECT q.text as input, q.answers[0].text as output,\n",
        "FROM yaqs.questions_cloud.latest AS q\n",
        "WHERE ARRAY_LENGTH(q.answers)>0 AND\n",
        "  (REGEXP_CONTAINS(q.text, 'vertex') OR REGEXP_CONTAINS(q.text, 'workbench') OR REGEXP_CONTAINS(q.text, 'jupyter') OR\n",
        "  REGEXP_CONTAINS(q.text, 'notebook') OR REGEXP_CONTAINS(q.text, 'vertexai') OR REGEXP_CONTAINS(q.text, 'aiplatform'))\n",
        "ORDER BY q.creation.time DESC\n",
        "LIMIT 1000;*\n",
        "\n",
        "The output of the query was saved on a json file names \"yaqs-examples.json\"\n",
        "\n"
      ],
      "metadata": {
        "id": "dDNmeg1XA45n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect colab with Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzTAUeYfF3zT",
        "outputId": "aab8bf7a-cf6d-4305-d7af-3b4a391256c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some ways to sign the answer\n",
        "signs = ['Glad to assist! TechBot',\n",
        "          'Thanks! TechBot',\n",
        "          'I am here to help! TechBot',\n",
        "          'Happy to provide support! TechBot',\n",
        "          'If you need any additional assistance, feel free to ask! TechBot',\n",
        "          'Cheers, TechBot']"
      ],
      "metadata": {
        "id": "qgLIbAyqcMS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "json_file_path = '/content/drive/MyDrive/q&a_project/yaqs-examples-1000.json'\n",
        "\n",
        "# Open the JSON file\n",
        "with open(json_file_path) as file:\n",
        "    # Load the JSON data\n",
        "    data = json.load(file)\n",
        "\n",
        "# Remove the first element (line) that are the column names.\n",
        "data = data[1:]\n",
        "\n",
        "# Reduce to 50 examples (prompt tuning has length limits)\n",
        "data = data[:60]\n",
        "\n",
        "EXAMPLES = []\n",
        "\n",
        "# Iterate over each row in the JSON data\n",
        "for row in data:\n",
        "  # Create line with the input and output information for each question in the json data\n",
        "  if(len(row[0])<1018 and len(row[1])<1015): # Max length of input/output field. Discard the bigger ones\n",
        "    new_input = \"user: \" + row[0]\n",
        "    new_output = f\"\"\"TechBot: {row[1]}\\n{random.choice(signs)}\"\"\"\n",
        "    EXAMPLES.append(new_input + \"\\n\" + new_output + \"\\n\")\n",
        "\n",
        "# Print info about the examples to use in the prompt\n",
        "print(\"--- We have \", len(EXAMPLES), \"examples.---\")\n",
        "print(\"---\",len(data) - len(EXAMPLES), \"discarded examples because of length.---\")\n",
        "#print(\" \".join(EXAMPLES)) # Print content of all examples\n",
        "\n",
        "# Input token limit: 8196\n",
        "# Note: For the PaLM 2 model, token is equivalent to about 4 characters. 100 tokens are about 60-80 English words.\n",
        "print(\"---\", len(\" \".join(EXAMPLES))/4, \"< 8196 that is the token limit for the current model.\", \"---\")\n"
      ],
      "metadata": {
        "id": "b7CnQZkH_PuI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c85e8cb8-e277-44ff-a393-60688a4b1473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- We have  25 examples.---\n",
            "--- 35 discarded examples because of length.---\n",
            "--- 6233.0 < 8196 that is the token limit for the current model. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Import and Initialize Text Model"
      ],
      "metadata": {
        "id": "Te9roXzHP7Nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.preview.language_models import TextGenerationModel, InputOutputTextPair\n",
        "\n",
        "vertexai.init(project = PROJECT_ID, location = LOCATION)\n",
        "\n",
        "# create text model\n",
        "text_model = TextGenerationModel.from_pretrained(MODEL)"
      ],
      "metadata": {
        "id": "9J17Rh-n0Q8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.0. Model parameters"
      ],
      "metadata": {
        "id": "Fkkn_2zz9Fuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Temperature controls the degree of randomness in token selection.\n",
        "TEMPERATURE = 0.025 # @param {type:\"number\"}\n",
        "\n",
        "# Tokens are selected from most probable to least until the sum of their probabilities equals the top_p value.\n",
        "TOP_P = 0.8 # @param {type:\"number\"}\n",
        "\n",
        "# A top_k of 1 means the selected token is the most probable among all tokens.\n",
        "TOP_K = 5 # @param {type:\"number\"}\n",
        "\n",
        "# Token limit determines the maximum amount of text output.\n",
        "MAX_OUTPUT_TOKENS = 1024 # @param {type:\"number\"}\n"
      ],
      "metadata": {
        "id": "25KgJeSXhA5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {\n",
        "    \"max_output_tokens\": MAX_OUTPUT_TOKENS,\n",
        "    \"temperature\": TEMPERATURE,\n",
        "    \"top_k\": TOP_K,\n",
        "    \"top_p\": TOP_P,\n",
        "}"
      ],
      "metadata": {
        "id": "_lg7aK7vZaw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CONTEXT = \"\"\"You are a dedicated technical virtual assistant, equipped with extensive knowledge of AI and the Google Cloud Platform (GCP).\n",
        "              When you know the user name, add it at the beginning of the answer. Add your name \"TechBot\" at the end.\n",
        "              If you are not sure then ask for more information or say you do not know.\n",
        "          \"\"\""
      ],
      "metadata": {
        "id": "93YPHT0DGCIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION = \"Hi I am Florencia,  I'm currently going through the automl notebooks to test how it works for some retail clients. Everything imports correctly except for the automl table client. I keep encountering import errors. For example, I can call automl.AutoMlClient() without any issues, but when I try to call automl.TablesClient, I receive the following error message:  arduino Copy code AttributeError: module 'google.cloud.automl_v1beta1' has no attribute 'TablesClient' Do you have any idea what might be causing this? Is it a known issue?\" # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "qy0cv5myUUvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(question, context, examples):\n",
        "  PROMPT = f\"\"\"\n",
        "    Context: {context}.\n",
        "\n",
        "    Answer the question below using the context provided:\n",
        "    {examples}\n",
        "    user: {question}\n",
        "    TechBot:\n",
        "    \"\"\"\n",
        "  return PROMPT"
      ],
      "metadata": {
        "id": "nA6gPeliZMk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Zero-shot prompt"
      ],
      "metadata": {
        "id": "wN0xJQN7JFYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = create_prompt(QUESTION, CONTEXT, \"\") # No examples"
      ],
      "metadata": {
        "id": "iUkOBAtoJIqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_model = text_model.predict(\n",
        "    prompt=PROMPT,\n",
        "    **parameters\n",
        ")\n",
        "zero_shot_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xHd04MiJPBU",
        "outputId": "1afc2af5-b767-4aac-b1ad-edd625717672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Hi Florencia,  I'm sorry to hear that you are having trouble with the automl table client. I have not encountered this issue before, but I will do some research and see if I can find a solution. In the meantime, you can try the following:\n",
              "\n",
              "1. Make sure that you are using the latest version of the automl library.\n",
              "2. Try reinstalling the automl library.\n",
              "3. Try creating a new notebook and importing the automl library.\n",
              "\n",
              "If none of these solutions work, please let me know and I will continue to investigate."
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Few-shot prompt"
      ],
      "metadata": {
        "id": "2yhTqWJdI3Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = create_prompt(QUESTION, CONTEXT, EXAMPLES)"
      ],
      "metadata": {
        "id": "Q2EbZCa1TLt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_model = text_model.predict(\n",
        "    prompt=PROMPT,\n",
        "    **parameters\n",
        ")\n",
        "few_shot_model"
      ],
      "metadata": {
        "id": "9SPGUoD2Sf9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa5a18c9-bba6-45db-c23f-d4345dad3dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Hi Florencia,\n",
              "\n",
              "I'm sorry to hear that you're having trouble with the automl table client. I've looked into the issue and it seems like it's a known issue. The automl table client is currently in beta and there are some known issues with it. I've added a bug to our tracker and we're working on a fix. In the meantime, you can use the automl v1 client instead.\n",
              "\n",
              "Here's a link to the bug: https://b.corp.google.com/issues/278573282\n",
              "\n",
              "Here's a link to the automl v1 client documentation: https://cloud.google.com/automl/docs/reference/rest/v1/projects.locations.models\n",
              "\n",
              "I hope this helps!\n",
              "\n",
              "Best,\n",
              "TechBot"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Fine Tuning"
      ],
      "metadata": {
        "id": "iYKGMDlDfeWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.1. Prepare data\n",
        "The JSONL format is required as input to fine-tune the model"
      ],
      "metadata": {
        "id": "KX8Gw8a7bXOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Set path parameters"
      ],
      "metadata": {
        "id": "ORyEVrtZeyCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the paths to the JSON and JSONL files\n",
        "json_file_path = '/content/drive/MyDrive/q&a_project/yaqs-examples-1000.json'  # @param {type:\"string\"}\n",
        "jsonl_file_path = '/content/drive/MyDrive/q&a_project/dataset-1000.jsonl'   # @param {type:\"string\"}\n",
        "train_data_path = '/content/drive/MyDrive/q&a_project/train-data.jsonl'   # @param {type:\"string\"}\n",
        "test_data_path = '/content/drive/MyDrive/q&a_project/test-data.jsonl'   # @param {type:\"string\"}\n",
        "\n",
        "# Nombre del bucket de GCS\n",
        "bucket_name = \"bucket_cfm\"   # @param {type:\"string\"}\n",
        "# Nombre del archivo en el bucket\n",
        "target_train_file_name = \"train_data_1000.jsonl\"  # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "y8eL03TUdhOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convert json to jsonl\n",
        "JSONL is the erquired format to tune the model."
      ],
      "metadata": {
        "id": "RCwTyeP_fPPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "def convert_json_to_jsonl(json_file_path, jsonl_file_path):\n",
        "  with open(json_file_path, 'r') as json_file, open(jsonl_file_path, 'w') as jsonl_file:\n",
        "      # Load JSON data\n",
        "      data = json.load(json_file)\n",
        "\n",
        "      # Remove the first element (line) that are the column names.\n",
        "      data = data[1:]\n",
        "\n",
        "      # Iterate over JSON objects\n",
        "      for obj in data:\n",
        "        try:\n",
        "          # Remove newlines from obj[0] and obj[1]\n",
        "          input_text = obj[0].replace('\\n', '')\n",
        "          output_text = obj[1].replace('\\n', '')\n",
        "\n",
        "          # Replace \" with ´ because the jsonl doesn't recognize the character\n",
        "          input_text = input_text.replace('\"', '´')\n",
        "          output_text = output_text.replace('\"', '´')\n",
        "\n",
        "          # Generate a random sign and add to the end of the answer\n",
        "          output_text = output_text + '\\n' + random.choice(signs)\n",
        "\n",
        "          # Write each object as a separate line in the JSONL file\n",
        "          json_line = json.dumps({\"input_text\": input_text, \"output_text\": output_text}) + '\\n'\n",
        "          jsonl_file.write(json_line)\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "          # Handle invalid JSON lines here if needed\n",
        "          print(\"ignore line\")\n",
        "          pass\n",
        "\n",
        "  print(\"Conversion complete.\")\n",
        "\n",
        "# Convert JSON to JSONL\n",
        "convert_json_to_jsonl(json_file_path, jsonl_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCWyRodlUYuz",
        "outputId": "b67472c4-9f6f-4598-f69b-cc99d5f84359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversion complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.2. Split the data\n",
        "Split dataset into training and validation sets."
      ],
      "metadata": {
        "id": "-MkQgALJvIo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the split ratio\n",
        "train_ratio = 0.8  # 80% para entrenamiento, 20% para validación\n",
        "\n",
        "# Load the JSONL file\n",
        "data = []\n",
        "with open(jsonl_file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "# Shuffle the data\n",
        "random.shuffle(data)\n",
        "\n",
        "# Calculate the split indices\n",
        "train_size = int(len(data) * train_ratio)\n",
        "train_data = data[:train_size]\n",
        "val_data = data[train_size:]\n",
        "\n",
        "# Save the training and validation sets to separate files\n",
        "with open(train_data_path, 'w') as train_file:\n",
        "    for item in train_data:\n",
        "        train_file.write(json.dumps(item) + '\\n')\n",
        "\n",
        "with open(test_data_path, 'w') as val_file:\n",
        "    for item in val_data:\n",
        "        val_file.write(json.dumps(item) + '\\n')\n",
        "\n",
        "print('done.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1cMUj_hvhwR",
        "outputId": "d7617912-2524-4b6e-c6b3-c86ebbb5b98a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Upload JSONL local file to GCS"
      ],
      "metadata": {
        "id": "3-J0rhDxfgo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "def upload_jsonl_to_gcs(bucket_name, local_file_path, target_train_file_name):\n",
        "    \"\"\"Uploads a local JSONL file to a Google Cloud Storage bucket.\"\"\"\n",
        "    # Instantiates a client\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Get the bucket\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Upload the file to the bucket\n",
        "    blob = bucket.blob(target_train_file_name)\n",
        "    blob.upload_from_filename(local_file_path)\n",
        "\n",
        "    print(f\"File {local_file_path} uploaded to {bucket_name}/{target_train_file_name}\")\n",
        "\n",
        "# Llamada a la función para cargar el archivo en el bucket\n",
        "upload_jsonl_to_gcs(bucket_name, train_data_path, target_train_file_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgD16RjIcVVX",
        "outputId": "0cac77b5-976c-422c-e35c-17577c3eab5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File /content/drive/MyDrive/q&a_project/train-data.jsonl uploaded to bucket_cfm/train_data_1000.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.3. Create fine-tuned model"
      ],
      "metadata": {
        "id": "9bPulVlNcARj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model to be tuned\n",
        "\n",
        "model_display_name = 'techbot_q&a'\n",
        "tuned_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
      ],
      "metadata": {
        "id": "_qMOaMR8PQPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tune the model (note: this may take up to an hour)\n",
        "\"\"\"Tune a new model, based on a prompt-response data.\n",
        "\n",
        "  \"training_data\" can be either the GCS URI of a file formatted in JSONL format\n",
        "  (for example: training_data=f'gs://{bucket}/{filename}.jsonl'), or a pandas\n",
        "  DataFrame. Each training example should be JSONL record with two keys, for\n",
        "  example:\n",
        "    {\n",
        "      \"input_text\": <input prompt>,\n",
        "      \"output_text\": <associated output>\n",
        "    },\n",
        "  or the pandas DataFame should contain two columns:\n",
        "    ['input_text', 'output_text']\n",
        "  with rows for each training example.\n",
        "\n",
        "  Args:\n",
        "    tuned_model_location: GCP Region, used to initialize aiplatform\n",
        "    training_data: GCS URI of training file or pandas dataframe of training data\n",
        "    train_steps: Number of training steps to use when tuning the model.\n",
        "  \"\"\"\n",
        "tuned_model.tune_model(\n",
        "    training_data= f'gs://{bucket_name}/{target_train_file_name}',\n",
        "    train_steps=100,\n",
        "    tuning_job_location=\"europe-west4\",\n",
        "    tuned_model_location=\"us-central1\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oln3AdDIPH9u",
        "outputId": "4e966614-d6d1-4c9d-9cc4-3a79dfd3ec34"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating PipelineJob\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PipelineJob created. Resource name: projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To use this PipelineJob in another session:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pipeline_job = aiplatform.PipelineJob.get('projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911')\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911')\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View Pipeline Job:\n",
            "https://console.cloud.google.com/vertex-ai/locations/europe-west4/pipelines/runs/tune-large-model-20230621000911?project=308351622118\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
            "https://console.cloud.google.com/vertex-ai/locations/europe-west4/pipelines/runs/tune-large-model-20230621000911?project=308351622118\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PipelineJob run completed. Resource name: projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/308351622118/locations/europe-west4/pipelineJobs/tune-large-model-20230621000911\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning has completed. Created Vertex Model: projects/308351622118/locations/us-central1/models/4840694499283828736\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:vertexai.language_models._language_models:Tuning has completed. Created Vertex Model: projects/308351622118/locations/us-central1/models/4840694499283828736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get deployed model to use\n",
        "tuned_model_name = \"projects/308351622118/locations/us-central1/models/4840694499283828736\"\n",
        "tuned_model = TextGenerationModel.from_pretrained(MODEL).get_tuned_model(tuned_model_name)\n",
        "tuned_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaIq3i8RVMsA",
        "outputId": "811966bb-d387-4d27-db99-b21c909a372e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<vertexai.language_models._language_models._PreviewTextGenerationModel at 0x7fb30442ec80>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate prompt to run the tuned model\n",
        "PROMPT = create_prompt(QUESTION, CONTEXT, \"\") # No examples because they are in the tuned model"
      ],
      "metadata": {
        "id": "aJ26P1sAZ0dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the model\n",
        "tuned_model.predict(\n",
        "    prompt= PROMPT,\n",
        "    max_output_tokens = MAX_OUTPUT_TOKENS,\n",
        "    temperature = TEMPERATURE,\n",
        "    top_p = TOP_P,\n",
        "    top_k = TOP_K,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b__Pn_iXioQ",
        "outputId": "b4b179e6-ffa2-45d4-a198-2c1242e8f0df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Hi Florencia, \n",
              "\n",
              "I'm sorry to hear that you are having trouble with the automl table client. I have not seen this issue before, but I will do some research and see if I can find a solution. In the meantime, you can try the following:\n",
              "\n",
              "1. Make sure that you are using the latest version of the automl library.\n",
              "2. Try reinstalling the automl library.\n",
              "3. Try creating a new notebook and importing the automl library.\n",
              "\n",
              "If none of these solutions work, please let me know and I will continue to investigate."
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Evaluation\n",
        "We will run all the questions in the test set and then compare the answer given by the Agent with the answer we have in the dataset."
      ],
      "metadata": {
        "id": "FsXwbH_zicqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1.Human Evaluation"
      ],
      "metadata": {
        "id": "bGPzrkOUviY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert JSONL to DataFrame"
      ],
      "metadata": {
        "id": "5QSM8hlWiYzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Read the JSONL file and parse each line\n",
        "data = []\n",
        "\n",
        "# We want to get 10 responses\n",
        "num_responses = 25\n",
        "\n",
        "with open(jsonl_file_path, 'r') as file:\n",
        "    i = 0\n",
        "    for line in file:\n",
        "      if i < num_responses:\n",
        "        json_object = json.loads(line)\n",
        "        user_question = json_object['input_text']\n",
        "        given_answer = json_object['output_text']\n",
        "\n",
        "\n",
        "        try:\n",
        "          # Add answer given by the zero-shot Agent\n",
        "          zero_shot_model = text_model.predict(\n",
        "                                  prompt=create_prompt(user_question, CONTEXT, \"\"),\n",
        "                                  **parameters\n",
        "                              )\n",
        "          # Add answer given by the few-shot Agent\n",
        "          few_shot_model = text_model.predict(\n",
        "                                  prompt=create_prompt(user_question, CONTEXT, EXAMPLES),\n",
        "                                  **parameters\n",
        "                              )\n",
        "\n",
        "          # Add answer given by the Tuned Agent\n",
        "          tuned_agent_answer = tuned_model.predict(\n",
        "                                prompt=create_prompt(user_question, CONTEXT, \"\"),\n",
        "                                **parameters\n",
        "                              )\n",
        "          data.append({'user_question': user_question, 'given_answer': given_answer, 'zero_shot_answer': zero_shot_model,\n",
        "                       'few_shot_answer': few_shot_model, 'tuned_agent_answer': tuned_agent_answer})\n",
        "          i += 1\n",
        "        except:\n",
        "          print(\"error\")\n",
        "          continue\n",
        "\n",
        "      else:\n",
        "            break  # Exit the loop once the desired number of elements have been processed\n",
        "\n",
        "# Create a DataFrame from the parsed data\n",
        "df = pd.DataFrame(data, columns=['user_question', 'given_answer', 'zero_shot_answer', 'few_shot_answer', 'tuned_agent_answer'])\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "knvWR0sTibcN",
        "outputId": "420d278d-3fed-4543-c0c2-4d8205152ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        user_question  \\\n",
              "0   I am reaching out to you to clarify/confirm if...   \n",
              "1   IHAC that their daily/monthly active user coun...   \n",
              "2   Dataflow Team, This is a question from a custo...   \n",
              "3   Question: I got a question passed on that the ...   \n",
              "4   Hello experts,My customer want to start using ...   \n",
              "5   Question:Kubernetes nodes have been restarted....   \n",
              "6   **Question:**----------Direct question from th...   \n",
              "7   Due to BQ Editions I am starting to get more c...   \n",
              "8   I created a CL(cl/536444816) to add new MAF te...   \n",
              "9   Hello there! We're adding VPCSC to our lists o...   \n",
              "10  Trying to run Cloud Profiling on this pipeline...   \n",
              "11  Our gdutst flume jobs are unable to read HFA p...   \n",
              "12  Will IAM support the [CreateServiceAccountKey]...   \n",
              "13  I have a question about doing Feature Engineer...   \n",
              "14  Posting the question that was asked by zihaoli...   \n",
              "15  Customer has a stream from CloudSQL to BigQuer...   \n",
              "16  Question:---I just got this question from a cl...   \n",
              "17  **Question:**----------Customer organises depl...   \n",
              "18  Our client is using export to splunk dataflow ...   \n",
              "19  Customer has multiple BMS servers, the servers...   \n",
              "20  **Question:**----------Our goal is to migrate ...   \n",
              "21  I have a customer that needs to spin up/down G...   \n",
              "22  I have the following line in the constructor o...   \n",
              "23  Hi team,My customer would like to use kube_pod...   \n",
              "24  Hi, IHAC that is using a FileIO.MatchAll and i...   \n",
              "\n",
              "                                         given_answer  \\\n",
              "0   I think generally, we advise against creating ...   \n",
              "1   One thing you could do is to have them turn on...   \n",
              "2   JdbcIO is not supported as a source connector ...   \n",
              "3   Hi Lieze,The select google customers refers to...   \n",
              "4   Hi Mikael, The Datastream Oracle CDC solution ...   \n",
              "5   Hey Jessica,Please forward this documentation ...   \n",
              "6   Hi Tom,This use case seems to be a good fit fo...   \n",
              "7   Which info schema table are you trying to use ...   \n",
              "8   Discussed with Sijun offline. Resolved the iss...   \n",
              "9   1. vpcsc-restricted-vip should not behave any ...   \n",
              "10  If you see a link to the profiler in the job p...   \n",
              "11  Did you specify --keystore_backend=DEV_LITE du...   \n",
              "12  As far as I can see, yes, we will support that...   \n",
              "13  Given that the source is Kafka there may be mo...   \n",
              "14  Hi Robert,From what I understand 1) RPC Step d...   \n",
              "15  The event size limitation is 10mb for bigquery...   \n",
              "16  I didn't get any response from the VertexAI te...   \n",
              "17  Buenas,En Google Cloud existe la opción de des...   \n",
              "18  This feature is not available out of the box o...   \n",
              "19  If customer needs isolation between VLANs in B...   \n",
              "20  HI Katharina,Can you please confirm whether th...   \n",
              "21  The customer's actions should work as they des...   \n",
              "22  It depends on which path is taken to view the ...   \n",
              "23  Hi Bruce,I noticed the related email thread (h...   \n",
              "24  Afraid we don't have a very good solution yet....   \n",
              "\n",
              "                                     zero_shot_answer  \\\n",
              "0   Hi user, thanks for reaching out. I'm glad to ...   \n",
              "1   user, I see that you are having some issues wi...   \n",
              "2   Hi user,\\n\\nI'm not sure if it's okay to sugge...   \n",
              "3   Hi Cacholong,\\n\\nI'm sorry, but I don't have a...   \n",
              "4   Hi Mikael,\\n\\nI'm not an Oracle expert either,...   \n",
              "5   Hi Compliance Srl,\\n\\nI'm sorry to hear that y...   \n",
              "6   Hi Fejron, thanks for your question. I'm not s...   \n",
              "7   Hi user, thanks for your question. I'm not sur...   \n",
              "8   user, I'm sorry to hear that you are having tr...   \n",
              "9   Hello Rafael, thanks for reaching out. I'm sor...   \n",
              "10  Hi user, I'm sorry to hear that you're having ...   \n",
              "11  Hi user, I'm sorry to hear that your gdutst fl...   \n",
              "12  Daniel, I am not sure if IAM will support the ...   \n",
              "13  Hi user, I'm glad to help you with your questi...   \n",
              "14  Hi zihaolin,\\n\\nThanks for your question. I'm ...   \n",
              "15  Hi user, thanks for your question.\\n\\nThe even...   \n",
              "16  Hi, I'm TechBot, your virtual assistant. I'm h...   \n",
              "17  Hi Loome.Health,\\n\\nYes, Google Cloud Platform...   \n",
              "18  Hi user,\\n\\nYes, there is a way to periodicall...   \n",
              "19  Yes, you need to create multiple partner inter...   \n",
              "20  Hi Histix.de,\\n\\nI'm sorry to hear that you're...   \n",
              "21  user, I'm not sure what the relevant “rules” a...   \n",
              "22  I am not sure which logs files should be searc...   \n",
              "23  Hi user,\\n\\nThanks for reaching out. I'm not s...   \n",
              "24  Hi user, thanks for reaching out. I'm sorry to...   \n",
              "\n",
              "                                      few_shot_answer  \\\n",
              "0   Hi,\\n\\nI'm not sure I understand the question....   \n",
              "1   Hi,\\n\\nI'm sorry to hear that you're experienc...   \n",
              "2   Hi,\\n\\nI would recommend that the customer use...   \n",
              "3   Hi,\\n\\nI'm sorry, but I can't answer this ques...   \n",
              "4   Hi Mikael,\\n\\n    I'm not an Oracle expert eit...   \n",
              "5   Hi Jessica,\\n\\nI understand that you are havin...   \n",
              "6   Hi,\\n\\nI hope you are well.\\n\\nI'm happy to he...   \n",
              "7   Hi,\\n\\nI'm sorry to hear that you're having tr...   \n",
              "8   Hi,\\n\\nI'm sorry to hear that you're having tr...   \n",
              "9   Hi Rafael,\\n\\n    Thanks for reaching out! I'm...   \n",
              "10  Hi,\\n\\nI'm sorry to hear that you're having tr...   \n",
              "11  Hi,\\n\\nI'm sorry to hear that you're having tr...   \n",
              "12  Hi Daniel,\\n\\nI think the best way to answer t...   \n",
              "13  Hi,\\n\\nI think you are right about the behavio...   \n",
              "14  Hi Zihao,\\n\\nThanks for bringing this up. I'm ...   \n",
              "15  The event size limitation is 20MB applied to a...   \n",
              "16  Hi,\\n\\nI'm sorry to hear that you're having tr...   \n",
              "17  Hi,\\n\\nYes, Google Cloud has APIs available to...   \n",
              "18  This feature is not available out of the box o...   \n",
              "19  Yes, you need to create multiple partner inter...   \n",
              "20  Hi,\\n    I hope you are well.\\n    I'm sorry t...   \n",
              "21  Hi,\\n\\nI'm sorry to hear that you're having tr...   \n",
              "22  The logs you are looking for are in the `/debu...   \n",
              "23  Hi Bruce,\\n\\n    I noticed the related email t...   \n",
              "24  Hi,\\n\\nI'm sorry to hear that you're experienc...   \n",
              "\n",
              "                                   tuned_agent_answer  \n",
              "0   Hi user,\\n\\nI'm not sure if I understand your ...  \n",
              "1   Hi IHAC,\\n\\n    I'm sorry to hear that you're ...  \n",
              "2   Hi Dataflow Team,\\n\\n    I'm not sure if it's ...  \n",
              "3   Hi Cacho,\\n\\n    Thanks for reaching out to Go...  \n",
              "4   Hi Melika,\\n\\nI'm sorry to hear that you're ha...  \n",
              "5   Hi Compliance Srl,\\n\\nI'm sorry to hear that y...  \n",
              "6   Hi Frenj,\\n\\nI'm not sure what you're asking f...  \n",
              "7   Hi user,\\n\\nI'm sorry, but I don't have enough...  \n",
              "8   Hi user, I'm sorry to hear that you're having ...  \n",
              "9   Hi Rafael,\\n\\nI'm sorry to hear that you're ha...  \n",
              "10  Hi Goutam,\\n\\n    I'm sorry, but I don't have ...  \n",
              "11  Hi user, I'm sorry to hear that you're having ...  \n",
              "12  Daniel,\\n\\n    I am not sure if IAM will suppo...  \n",
              "13  Hi user, I am not sure if I understand your qu...  \n",
              "14  Hi zihaolin,\\n\\n    I'm not sure if it's safe ...  \n",
              "15  Hi user,\\n\\n    The event size limit is applie...  \n",
              "16  Hi [user name],\\n\\n    I'm not sure when image...  \n",
              "17  Hi Loome.Health,\\n\\n    Yes, Google Cloud Plat...  \n",
              "18  Hi user, I'm not sure if I understand your que...  \n",
              "19  Hi user,\\n\\n    I understand that you have a c...  \n",
              "20  Hi Histix.de,\\n\\n    I'm sorry to hear that yo...  \n",
              "21  Hi team,\\n\\nI'm not sure what the relevant rul...  \n",
              "22  Hi user, I'm not sure what you're asking. Can ...  \n",
              "23  Hi user,\\n\\n    I'm not sure if we are not sup...  \n",
              "24  Hi, thanks for reaching out. I'm sorry to hear...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6b9d52a7-5da2-4507-a399-6369322fdea3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_question</th>\n",
              "      <th>given_answer</th>\n",
              "      <th>zero_shot_answer</th>\n",
              "      <th>few_shot_answer</th>\n",
              "      <th>tuned_agent_answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I am reaching out to you to clarify/confirm if...</td>\n",
              "      <td>I think generally, we advise against creating ...</td>\n",
              "      <td>Hi user, thanks for reaching out. I'm glad to ...</td>\n",
              "      <td>Hi,\\n\\nI'm not sure I understand the question....</td>\n",
              "      <td>Hi user,\\n\\nI'm not sure if I understand your ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IHAC that their daily/monthly active user coun...</td>\n",
              "      <td>One thing you could do is to have them turn on...</td>\n",
              "      <td>user, I see that you are having some issues wi...</td>\n",
              "      <td>Hi,\\n\\nI'm sorry to hear that you're experienc...</td>\n",
              "      <td>Hi IHAC,\\n\\n    I'm sorry to hear that you're ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dataflow Team, This is a question from a custo...</td>\n",
              "      <td>JdbcIO is not supported as a source connector ...</td>\n",
              "      <td>Hi user,\\n\\nI'm not sure if it's okay to sugge...</td>\n",
              "      <td>Hi,\\n\\nI would recommend that the customer use...</td>\n",
              "      <td>Hi Dataflow Team,\\n\\n    I'm not sure if it's ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Question: I got a question passed on that the ...</td>\n",
              "      <td>Hi Lieze,The select google customers refers to...</td>\n",
              "      <td>Hi Cacholong,\\n\\nI'm sorry, but I don't have a...</td>\n",
              "      <td>Hi,\\n\\nI'm sorry, but I can't answer this ques...</td>\n",
              "      <td>Hi Cacho,\\n\\n    Thanks for reaching out to Go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hello experts,My customer want to start using ...</td>\n",
              "      <td>Hi Mikael, The Datastream Oracle CDC solution ...</td>\n",
              "      <td>Hi Mikael,\\n\\nI'm not an Oracle expert either,...</td>\n",
              "      <td>Hi Mikael,\\n\\n    I'm not an Oracle expert eit...</td>\n",
              "      <td>Hi Melika,\\n\\nI'm sorry to hear that you're ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Question:Kubernetes nodes have been restarted....</td>\n",
              "      <td>Hey Jessica,Please forward this documentation ...</td>\n",
              "      <td>Hi Compliance Srl,\\n\\nI'm sorry to hear that y...</td>\n",
              "      <td>Hi Jessica,\\n\\nI understand that you are havin...</td>\n",
              "      <td>Hi Compliance Srl,\\n\\nI'm sorry to hear that y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>**Question:**----------Direct question from th...</td>\n",
              "      <td>Hi Tom,This use case seems to be a good fit fo...</td>\n",
              "      <td>Hi Fejron, thanks for your question. I'm not s...</td>\n",
              "      <td>Hi,\\n\\nI hope you are well.\\n\\nI'm happy to he...</td>\n",
              "      <td>Hi Frenj,\\n\\nI'm not sure what you're asking f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Due to BQ Editions I am starting to get more c...</td>\n",
              "      <td>Which info schema table are you trying to use ...</td>\n",
              "      <td>Hi user, thanks for your question. I'm not sur...</td>\n",
              "      <td>Hi,\\n\\nI'm sorry to hear that you're having tr...</td>\n",
              "      <td>Hi user,\\n\\nI'm sorry, but I don't have enough...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>I created a CL(cl/536444816) to add new MAF te...</td>\n",
              "      <td>Discussed with Sijun offline. Resolved the iss...</td>\n",
              "      <td>user, I'm sorry to hear that you are having tr...</td>\n",
              "      <td>Hi,\\n\\nI'm sorry to hear that you're having tr...</td>\n",
              "      <td>Hi user, I'm sorry to hear that you're having ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Hello there! We're adding VPCSC to our lists o...</td>\n",
              "      <td>1. vpcsc-restricted-vip should not behave any ...</td>\n",
              "      <td>Hello Rafael, thanks for reaching out. I'm sor...</td>\n",
              "      <td>Hi Rafael,\\n\\n    Thanks for reaching out! I'm...</td>\n",
              "      <td>Hi Rafael,\\n\\nI'm sorry to hear that you're ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Trying to run Cloud Profiling on this pipeline...</td>\n",
              "      <td>If you see a link to the profiler in the job p...</td>\n",
              "      <td>Hi user, I'm sorry to hear that you're having ...</td>\n",
              "      <td>Hi,\\n\\nI'm sorry to hear that you're having tr...</td>\n",
              "      <td>Hi Goutam,\\n\\n    I'm sorry, but I don't have ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Our gdutst flume jobs are unable to read HFA p...</td>\n",
              "      <td>Did you specify --keystore_backend=DEV_LITE du...</td>\n",
              "      <td>Hi user, I'm sorry to hear that your gdutst fl...</td>\n",
              "      <td>Hi,\\n\\nI'm sorry to hear that you're having tr...</td>\n",
              "      <td>Hi user, I'm sorry to hear that you're having ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Will IAM support the [CreateServiceAccountKey]...</td>\n",
              "      <td>As far as I can see, yes, we will support that...</td>\n",
              "      <td>Daniel, I am not sure if IAM will support the ...</td>\n",
              "      <td>Hi Daniel,\\n\\nI think the best way to answer t...</td>\n",
              "      <td>Daniel,\\n\\n    I am not sure if IAM will suppo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>I have a question about doing Feature Engineer...</td>\n",
              "      <td>Given that the source is Kafka there may be mo...</td>\n",
              "      <td>Hi user, I'm glad to help you with your questi...</td>\n",
              "      <td>Hi,\\n\\nI think you are right about the behavio...</td>\n",
              "      <td>Hi user, I am not sure if I understand your qu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Posting the question that was asked by zihaoli...</td>\n",
              "      <td>Hi Robert,From what I understand 1) RPC Step d...</td>\n",
              "      <td>Hi zihaolin,\\n\\nThanks for your question. I'm ...</td>\n",
              "      <td>Hi Zihao,\\n\\nThanks for bringing this up. I'm ...</td>\n",
              "      <td>Hi zihaolin,\\n\\n    I'm not sure if it's safe ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Customer has a stream from CloudSQL to BigQuer...</td>\n",
              "      <td>The event size limitation is 10mb for bigquery...</td>\n",
              "      <td>Hi user, thanks for your question.\\n\\nThe even...</td>\n",
              "      <td>The event size limitation is 20MB applied to a...</td>\n",
              "      <td>Hi user,\\n\\n    The event size limit is applie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Question:---I just got this question from a cl...</td>\n",
              "      <td>I didn't get any response from the VertexAI te...</td>\n",
              "      <td>Hi, I'm TechBot, your virtual assistant. I'm h...</td>\n",
              "      <td>Hi,\\n\\nI'm sorry to hear that you're having tr...</td>\n",
              "      <td>Hi [user name],\\n\\n    I'm not sure when image...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>**Question:**----------Customer organises depl...</td>\n",
              "      <td>Buenas,En Google Cloud existe la opción de des...</td>\n",
              "      <td>Hi Loome.Health,\\n\\nYes, Google Cloud Platform...</td>\n",
              "      <td>Hi,\\n\\nYes, Google Cloud has APIs available to...</td>\n",
              "      <td>Hi Loome.Health,\\n\\n    Yes, Google Cloud Plat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Our client is using export to splunk dataflow ...</td>\n",
              "      <td>This feature is not available out of the box o...</td>\n",
              "      <td>Hi user,\\n\\nYes, there is a way to periodicall...</td>\n",
              "      <td>This feature is not available out of the box o...</td>\n",
              "      <td>Hi user, I'm not sure if I understand your que...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Customer has multiple BMS servers, the servers...</td>\n",
              "      <td>If customer needs isolation between VLANs in B...</td>\n",
              "      <td>Yes, you need to create multiple partner inter...</td>\n",
              "      <td>Yes, you need to create multiple partner inter...</td>\n",
              "      <td>Hi user,\\n\\n    I understand that you have a c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>**Question:**----------Our goal is to migrate ...</td>\n",
              "      <td>HI Katharina,Can you please confirm whether th...</td>\n",
              "      <td>Hi Histix.de,\\n\\nI'm sorry to hear that you're...</td>\n",
              "      <td>Hi,\\n    I hope you are well.\\n    I'm sorry t...</td>\n",
              "      <td>Hi Histix.de,\\n\\n    I'm sorry to hear that yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>I have a customer that needs to spin up/down G...</td>\n",
              "      <td>The customer's actions should work as they des...</td>\n",
              "      <td>user, I'm not sure what the relevant “rules” a...</td>\n",
              "      <td>Hi,\\n\\nI'm sorry to hear that you're having tr...</td>\n",
              "      <td>Hi team,\\n\\nI'm not sure what the relevant rul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>I have the following line in the constructor o...</td>\n",
              "      <td>It depends on which path is taken to view the ...</td>\n",
              "      <td>I am not sure which logs files should be searc...</td>\n",
              "      <td>The logs you are looking for are in the `/debu...</td>\n",
              "      <td>Hi user, I'm not sure what you're asking. Can ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Hi team,My customer would like to use kube_pod...</td>\n",
              "      <td>Hi Bruce,I noticed the related email thread (h...</td>\n",
              "      <td>Hi user,\\n\\nThanks for reaching out. I'm not s...</td>\n",
              "      <td>Hi Bruce,\\n\\n    I noticed the related email t...</td>\n",
              "      <td>Hi user,\\n\\n    I'm not sure if we are not sup...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Hi, IHAC that is using a FileIO.MatchAll and i...</td>\n",
              "      <td>Afraid we don't have a very good solution yet....</td>\n",
              "      <td>Hi user, thanks for reaching out. I'm sorry to...</td>\n",
              "      <td>Hi,\\n\\nI'm sorry to hear that you're experienc...</td>\n",
              "      <td>Hi, thanks for reaching out. I'm sorry to hear...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b9d52a7-5da2-4507-a399-6369322fdea3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6b9d52a7-5da2-4507-a399-6369322fdea3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6b9d52a7-5da2-4507-a399-6369322fdea3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Escribir el DataFrame en un archivo xlsx\n",
        "from datetime import datetime # para generar archivo de salida con fecha\n",
        "\n",
        "hora_actual = datetime.now().strftime(\"%d-%m-%H:%M\")\n",
        "\n",
        "df.to_excel(f'/content/drive/MyDrive/q&a_project/output-{hora_actual}.xlsx', index=False)"
      ],
      "metadata": {
        "id": "z6AJJIFxgad-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2.Apply ROUGE metric to the model\n",
        "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics commonly used to evaluate the quality of text summarization or machine translation outputs. Here's an outline of the process:"
      ],
      "metadata": {
        "id": "ErhjM9eVdeJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtoaUuyqdckY",
        "outputId": "64198f1d-8ee6-4be8-c9f3-8fbb06296d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "rouge = Rouge()"
      ],
      "metadata": {
        "id": "YrxmZ8PtdXDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[0, 'given_answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "5QRua5y3hEYK",
        "outputId": "97119fd5-7b82-4d4a-fad9-915bded0bcfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I think generally, we advise against creating custom roles because there could be unknown breakages with the service. Aaron might be able to add some more color.\\nI am here to help! TechBot'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "str(df.loc[0, 'few_shot_answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KALihSZsg9wy",
        "outputId": "96c66c4f-b3e1-41f6-d4a1-7c44af9c0976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hi,\\n\\nI'm not sure I understand the question. Can you please clarify what you're asking?\\n\\nThanks,\\nTechBot\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **scores[0]**: The ROUGE scores dictionary contains a list of score dictionaries.\n",
        "*   **['rouge-1']**: ROUGE-1 represents unigram overlap between the generated and reference text.\n",
        "*   **['f']**: Finally, within the 'rouge-1' score dictionary, you can access the 'f' key. 'f' corresponds to the F1-score, which is a harmonic mean of precision and recall. F1-score is commonly used as a measure of performance in tasks such as text summarization or machine translation.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZmcDlvW0uo_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "for i in range(25):\n",
        "  reference_answer = df.loc[i, 'given_answer']\n",
        "  generated_answer_zero = str(df.loc[i, 'zero_shot_answer']) #convert 'TextGenerationResponse' to string\n",
        "  generated_answer_few = str(df.loc[i, 'few_shot_answer']) #convert 'TextGenerationResponse' to string\n",
        "  generated_answer_tuned = str(df.loc[i, 'tuned_agent_answer'])\n",
        "  scores.append(rouge.get_scores(generated_answer_few, reference_answer))\n",
        "\n",
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-ujNUYTdkJg",
        "outputId": "3cb902e6-ca63-4ad1-db31-e76154afe282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'rouge-1': {'r': 0.1, 'p': 0.17647058823529413, 'f': 0.12765956985061133},\n",
              "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
              "   'rouge-l': {'r': 0.1, 'p': 0.17647058823529413, 'f': 0.12765956985061133}}],\n",
              " [{'rouge-1': {'r': 0.22, 'p': 0.14473684210526316, 'f': 0.1746031698160747},\n",
              "   'rouge-2': {'r': 0.017241379310344827,\n",
              "    'p': 0.00980392156862745,\n",
              "    'f': 0.012499995378126709},\n",
              "   'rouge-l': {'r': 0.22, 'p': 0.14473684210526316, 'f': 0.1746031698160747}}],\n",
              " [{'rouge-1': {'r': 0.09859154929577464, 'p': 0.25, 'f': 0.14141413735741262},\n",
              "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
              "   'rouge-l': {'r': 0.07042253521126761,\n",
              "    'p': 0.17857142857142858,\n",
              "    'f': 0.10101009695337228}}],\n",
              " [{'rouge-1': {'r': 0.21621621621621623,\n",
              "    'p': 0.16326530612244897,\n",
              "    'f': 0.18604650672525702},\n",
              "   'rouge-2': {'r': 0.023809523809523808,\n",
              "    'p': 0.016129032258064516,\n",
              "    'f': 0.019230764415681677},\n",
              "   'rouge-l': {'r': 0.1891891891891892,\n",
              "    'p': 0.14285714285714285,\n",
              "    'f': 0.1627906927717687}}],\n",
              " [{'rouge-1': {'r': 0.5, 'p': 0.14084507042253522, 'f': 0.21978021635068232},\n",
              "   'rouge-2': {'r': 0.13636363636363635,\n",
              "    'p': 0.025423728813559324,\n",
              "    'f': 0.04285714020816344},\n",
              "   'rouge-l': {'r': 0.475,\n",
              "    'p': 0.13380281690140844,\n",
              "    'f': 0.2087912053616713}}],\n",
              " [{'rouge-1': {'r': 0.22580645161290322,\n",
              "    'p': 0.08536585365853659,\n",
              "    'f': 0.12389380132821691},\n",
              "   'rouge-2': {'r': 0.05714285714285714,\n",
              "    'p': 0.01694915254237288,\n",
              "    'f': 0.026143787321116288},\n",
              "   'rouge-l': {'r': 0.22580645161290322,\n",
              "    'p': 0.08536585365853659,\n",
              "    'f': 0.12389380132821691}}],\n",
              " [{'rouge-1': {'r': 0.0967741935483871,\n",
              "    'p': 0.32727272727272727,\n",
              "    'f': 0.1493775898383293},\n",
              "   'rouge-2': {'r': 0.0037174721189591076,\n",
              "    'p': 0.015151515151515152,\n",
              "    'f': 0.005970146089732129},\n",
              "   'rouge-l': {'r': 0.08602150537634409,\n",
              "    'p': 0.2909090909090909,\n",
              "    'f': 0.13278007946488535}}],\n",
              " [{'rouge-1': {'r': 0.24242424242424243,\n",
              "    'p': 0.32786885245901637,\n",
              "    'f': 0.27874563971154204},\n",
              "   'rouge-2': {'r': 0.05204460966542751,\n",
              "    'p': 0.06666666666666667,\n",
              "    'f': 0.05845510989840567},\n",
              "   'rouge-l': {'r': 0.21212121212121213,\n",
              "    'p': 0.28688524590163933,\n",
              "    'f': 0.24390243413662915}}],\n",
              " [{'rouge-1': {'r': 0.375, 'p': 0.13636363636363635, 'f': 0.19999999608888894},\n",
              "   'rouge-2': {'r': 0.07407407407407407,\n",
              "    'p': 0.024691358024691357,\n",
              "    'f': 0.03703703328703741},\n",
              "   'rouge-l': {'r': 0.3333333333333333,\n",
              "    'p': 0.12121212121212122,\n",
              "    'f': 0.17777777386666674}}],\n",
              " [{'rouge-1': {'r': 0.3333333333333333,\n",
              "    'p': 0.27218934911242604,\n",
              "    'f': 0.2996742621519592},\n",
              "   'rouge-2': {'r': 0.05238095238095238,\n",
              "    'p': 0.043824701195219126,\n",
              "    'f': 0.04772233777273828},\n",
              "   'rouge-l': {'r': 0.3115942028985507,\n",
              "    'p': 0.25443786982248523,\n",
              "    'f': 0.28013028821059116}}],\n",
              " [{'rouge-1': {'r': 0.3, 'p': 0.1935483870967742, 'f': 0.23529411287966176},\n",
              "   'rouge-2': {'r': 0.038461538461538464,\n",
              "    'p': 0.023255813953488372,\n",
              "    'f': 0.028985502549885238},\n",
              "   'rouge-l': {'r': 0.25,\n",
              "    'p': 0.16129032258064516,\n",
              "    'f': 0.19607842660515198}}],\n",
              " [{'rouge-1': {'r': 0.4166666666666667,\n",
              "    'p': 0.05747126436781609,\n",
              "    'f': 0.1010100988797062},\n",
              "   'rouge-2': {'r': 0.09090909090909091,\n",
              "    'p': 0.00819672131147541,\n",
              "    'f': 0.01503759246763541},\n",
              "   'rouge-l': {'r': 0.4166666666666667,\n",
              "    'p': 0.05747126436781609,\n",
              "    'f': 0.1010100988797062}}],\n",
              " [{'rouge-1': {'r': 0.08333333333333333,\n",
              "    'p': 0.06451612903225806,\n",
              "    'f': 0.0727272678082648},\n",
              "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
              "   'rouge-l': {'r': 0.08333333333333333,\n",
              "    'p': 0.06451612903225806,\n",
              "    'f': 0.0727272678082648}}],\n",
              " [{'rouge-1': {'r': 0.2897196261682243,\n",
              "    'p': 0.23484848484848486,\n",
              "    'f': 0.25941422099613115},\n",
              "   'rouge-2': {'r': 0.04195804195804196,\n",
              "    'p': 0.0273972602739726,\n",
              "    'f': 0.033149166491102916},\n",
              "   'rouge-l': {'r': 0.27102803738317754,\n",
              "    'p': 0.2196969696969697,\n",
              "    'f': 0.24267781932249094}}],\n",
              " [{'rouge-1': {'r': 0.21487603305785125,\n",
              "    'p': 0.3132530120481928,\n",
              "    'f': 0.25490195595780474},\n",
              "   'rouge-2': {'r': 0.03428571428571429,\n",
              "    'p': 0.04838709677419355,\n",
              "    'f': 0.040133774409682806},\n",
              "   'rouge-l': {'r': 0.17355371900826447,\n",
              "    'p': 0.25301204819277107,\n",
              "    'f': 0.20588234811466755}}],\n",
              " [{'rouge-1': {'r': 0.1794871794871795,\n",
              "    'p': 0.32558139534883723,\n",
              "    'f': 0.23140495409603176},\n",
              "   'rouge-2': {'r': 0.05434782608695652,\n",
              "    'p': 0.09615384615384616,\n",
              "    'f': 0.06944443983024721},\n",
              "   'rouge-l': {'r': 0.15384615384615385,\n",
              "    'p': 0.27906976744186046,\n",
              "    'f': 0.1983471028563624}}],\n",
              " [{'rouge-1': {'r': 0.20689655172413793, 'p': 0.1, 'f': 0.13483145628077278},\n",
              "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
              "   'rouge-l': {'r': 0.1724137931034483,\n",
              "    'p': 0.08333333333333333,\n",
              "    'f': 0.11235954616841326}}],\n",
              " [{'rouge-1': {'r': 0.05228758169934641,\n",
              "    'p': 0.2962962962962963,\n",
              "    'f': 0.08888888633888897},\n",
              "   'rouge-2': {'r': 0.012244897959183673,\n",
              "    'p': 0.11538461538461539,\n",
              "    'f': 0.022140219667488324},\n",
              "   'rouge-l': {'r': 0.05228758169934641,\n",
              "    'p': 0.2962962962962963,\n",
              "    'f': 0.08888888633888897}}],\n",
              " [{'rouge-1': {'r': 0.847457627118644,\n",
              "    'p': 0.9090909090909091,\n",
              "    'f': 0.8771929774622961},\n",
              "   'rouge-2': {'r': 0.8428571428571429,\n",
              "    'p': 0.9076923076923077,\n",
              "    'f': 0.8740740690809329},\n",
              "   'rouge-l': {'r': 0.847457627118644,\n",
              "    'p': 0.9090909090909091,\n",
              "    'f': 0.8771929774622961}}],\n",
              " [{'rouge-1': {'r': 0.06779661016949153, 'p': 0.5, 'f': 0.11940298297170863},\n",
              "   'rouge-2': {'r': 0.005988023952095809,\n",
              "    'p': 0.0625,\n",
              "    'f': 0.010928960152886255},\n",
              "   'rouge-l': {'r': 0.06779661016949153, 'p': 0.5, 'f': 0.11940298297170863}}],\n",
              " [{'rouge-1': {'r': 0.21428571428571427,\n",
              "    'p': 0.140625,\n",
              "    'f': 0.16981131597009624},\n",
              "   'rouge-2': {'r': 0.019230769230769232,\n",
              "    'p': 0.010638297872340425,\n",
              "    'f': 0.013698625550761527},\n",
              "   'rouge-l': {'r': 0.21428571428571427,\n",
              "    'p': 0.140625,\n",
              "    'f': 0.16981131597009624}}],\n",
              " [{'rouge-1': {'r': 0.20689655172413793,\n",
              "    'p': 0.14457831325301204,\n",
              "    'f': 0.17021276111463218},\n",
              "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
              "   'rouge-l': {'r': 0.15517241379310345,\n",
              "    'p': 0.10843373493975904,\n",
              "    'f': 0.12765956962527056}}],\n",
              " [{'rouge-1': {'r': 0.12121212121212122,\n",
              "    'p': 0.11428571428571428,\n",
              "    'f': 0.1176470538278549},\n",
              "   'rouge-2': {'r': 0.027777777777777776,\n",
              "    'p': 0.02127659574468085,\n",
              "    'f': 0.024096380629990838},\n",
              "   'rouge-l': {'r': 0.12121212121212122,\n",
              "    'p': 0.11428571428571428,\n",
              "    'f': 0.1176470538278549}}],\n",
              " [{'rouge-1': {'r': 0.7222222222222222,\n",
              "    'p': 0.8666666666666667,\n",
              "    'f': 0.7878787829201102},\n",
              "   'rouge-2': {'r': 0.5789473684210527, 'p': 0.6875, 'f': 0.6285714236081633},\n",
              "   'rouge-l': {'r': 0.7222222222222222,\n",
              "    'p': 0.8666666666666667,\n",
              "    'f': 0.7878787829201102}}],\n",
              " [{'rouge-1': {'r': 0.21176470588235294,\n",
              "    'p': 0.15517241379310345,\n",
              "    'f': 0.17910447273087313},\n",
              "   'rouge-2': {'r': 0.009174311926605505,\n",
              "    'p': 0.006535947712418301,\n",
              "    'f': 0.007633582927280056},\n",
              "   'rouge-l': {'r': 0.21176470588235294,\n",
              "    'p': 0.15517241379310345,\n",
              "    'f': 0.17910447273087313}}]]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each ROUGE variant (ROUGE-1, ROUGE-2, and ROUGE-L) has associated recall, precision, and F1-score values. These scores indicate how well the generated summary matches the reference summary in terms of unigram overlap (ROUGE-1), bigram overlap (ROUGE-2), and the longest common subsequence (ROUGE-L). The higher the values, the better the match.\n",
        "\n",
        "For example, in the first element, the ROUGE-1 F1-score is 0.18, indicating a moderate level of agreement between the generated and reference summaries based on unigram overlap. Similarly, the other scores provide insights into the performance of the generated summaries for different ROUGE variants.\n",
        "\n"
      ],
      "metadata": {
        "id": "SmlZ7HIAwQ31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see that the 18th answer is the one with best results\n",
        "# Print both answers:\n",
        "print(\"reference_answer = \", df.loc[18, 'given_answer'])\n",
        "print(\"generated_answer = \", str(df.loc[18, 'few_shot_answer']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7qG8rjFwt4p",
        "outputId": "b6f036f6-1f82-46f0-b710-b68cbab1ce61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reference_answer =  This feature is not available out of the box on the Google-provided Pub-Sub to Splunk Template (though there is a FR for that tracked in b/205332203). A possible workaround for the client is to fork the open-source template and create their own custom template with additional logic to reload the UDF script periodically (it's currently cached forever [here](https://github.com/GoogleCloudPlatform/DataflowTemplates/blob/3c60bec4911735960496bf66e2030dc4aa84f2ce/v1/src/main/java/com/google/cloud/teleport/templates/common/JavascriptTextTransformer.java#L140)).\n",
            "If you need any additional assistance, feel free to ask! TechBot\n",
            "generated_answer =  This feature is not available out of the box on the Google-provided Pub-Sub to Splunk Template (though there is a FR for that tracked in b/205332203). A possible workaround for the client is to fork the open-source template and create their own custom template with additional logic to reload the UDF script periodically (it's currently cached forever [here](https://github.com/GoogleCloudPlatform/DataflowTemplates/blob/3c60bec4911735960496bf66e2030dc4aa84f2ce/v1/src/main/java/com/google/cloud/teleport/templates/common/JavascriptTextTransformer.java#L140)).\\n\\nI am here to help! TechBot\\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Efectively after compare results we can see that the answers are similars."
      ],
      "metadata": {
        "id": "tt-atwkjxSPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see another one with a relatively good metric value."
      ],
      "metadata": {
        "id": "-8e2KZo7s1jI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see that the 23th answer is the one with best results\n",
        "# Print both answers:\n",
        "print(\"reference_answer = \", df.loc[23, 'given_answer'])\n",
        "print(\"generated_answer = \", str(df.loc[23, 'few_shot_answer']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_Q61f9Gtftx",
        "outputId": "076a532c-72a2-44a7-f3a2-617fd2bbd11b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reference_answer =  Hi Bruce,I noticed the related email thread (http://shortn/_OcWJzUF93U), so I made the reply there.\n",
            "I am here to help! TechBot\n",
            "generated_answer =  Hi Bruce,\n",
            "\n",
            "    I noticed the related email thread (http://shortn/_OcWJzUF93U), so I made the reply there.\n",
            "\n",
            "    Thanks! TechBot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Run the model with a friendly UI"
      ],
      "metadata": {
        "id": "gePgBCTxZfeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def TechBot(user_question):\n",
        "    prompt = create_prompt(user_question, CONTEXT, EXAMPLES)\n",
        "\n",
        "    #change between tuned_model or text_model\n",
        "    model = text_model # @param {type:\"string\"}\n",
        "\n",
        "    prediction = model.predict(\n",
        "          prompt,\n",
        "          **parameters\n",
        "      )\n",
        "\n",
        "    return prediction\n"
      ],
      "metadata": {
        "id": "hLG2Uhc0XwSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Demo**"
      ],
      "metadata": {
        "id": "1GW-sQldyWyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.Interface(fn = TechBot,\n",
        "                     inputs=[gr.TextArea(label=\"Question\")],\n",
        "                     outputs = gr.TextArea(label=\"Answer\"),\n",
        "                     title = \"TechBot Q&A\",\n",
        "                     description = \"Enter a question and get the answer from a GCP expert.\",\n",
        "                     allow_flagging = False,\n",
        "                     )\n",
        "\n",
        "\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "_LPQb9CigXfT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "973bbb9e-dead-4b8d-c150-6eca34261961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/interface.py:326: UserWarning: The `allow_flagging` parameter in `Interface` nowtakes a string value ('auto', 'manual', or 'never'), not a boolean. Setting parameter to: 'never'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://bb084fd5c7ccbddbee.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://bb084fd5c7ccbddbee.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://bb084fd5c7ccbddbee.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **END :)**"
      ],
      "metadata": {
        "id": "o2BL8Bx3yaXF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cioagXYKy2sj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}